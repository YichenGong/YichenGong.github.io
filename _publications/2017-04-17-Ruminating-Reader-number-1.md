---
title: "Ruminating Reader: Reasoning with Gated Multi-hop Attention"
collection: publications
permalink: /publication/2009-10-01-paper-title-number-1
excerpt: 'This paper is about the number 1. The number 2 is left for future work.'
date: 2017-04-17
venue: 'ArXiv'
paperurl: 'https://arxiv.org/abs/1704.07415'
citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---
To answer the question in machine comprehension (MC) task, the models need to establish the interaction between the question and the context. To tackle the problem that the single-pass model cannot reflect on and correct its answer, we present Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BiDAF). We propose novel layer structures that construct an query-aware context vector representation and fuse encoding representation with intermediate representation on top of BiDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we find that the Reader outperforms the BiDAF baseline by a substantial margin, and matches or surpasses the performance of all other published systems.

[Paper Link](https://arxiv.org/abs/1704.07415)

Recommended citation:

	@article{Gong:2017wo,
		author = {Gong, Yichen and Bowman, Samuel R},
		title = {{Ruminating Reader: Reasoning with Gated Multi-Hop Attention}},
		journal = {arXiv.org},
		year = {2017},
		eprint = {1704.07415v1},
		eprinttype = {arxiv},
		eprintclass = {cs.CL},
		month = apr,
		annote = {10 pages, 6 figures}
	}

